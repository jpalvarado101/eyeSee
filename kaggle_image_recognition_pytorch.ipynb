{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57fda6f",
   "metadata": {},
   "source": [
    "\n",
    "# Kaggle Image Recognition (PyTorch) â€” Notebook\n",
    "This notebook trains an image classifier on a Kaggle-style dataset, automatically creating **train / val / test** splits and saving a best-model checkpoint. It also supports **your own custom model**.\n",
    "\n",
    "**Highlights**\n",
    "- Works with folder-per-class *or* CSV (`filepath,label`).\n",
    "- Stratified splitting (if scikit-learn is installed), otherwise random.\n",
    "- Transfer learning (ResNet50 / EfficientNet-B0) or your own `nn.Module`.\n",
    "- Mixed precision, cosine LR schedule, early stopping, gradient clipping.\n",
    "- Artifacts: `best_model.pt`, `class_to_idx.json`, `training_metrics.csv`, `split_*.csv`.\n",
    "- Inference on single image or directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== CONFIG ====\n",
    "# Set these paths according to your environment/dataset\n",
    "\n",
    "DATA_ROOT = '/path/to/dataset_root'  # folder with subfolders per class; or base root for CSV-relative paths\n",
    "CSV_PATH = None  # e.g. '/path/to/labels.csv' with columns: filepath,label (relative to DATA_ROOT or absolute); set to None for folder mode\n",
    "\n",
    "OUTPUT_DIR = 'runs/notebook_run'  # where to write artifacts\n",
    "SEED = 42\n",
    "\n",
    "# Split ratios\n",
    "VAL_SIZE = 0.10\n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "# Training\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "IMG_SIZE = 224\n",
    "AUG = True\n",
    "WORKERS = 4\n",
    "AMP = True\n",
    "PATIENCE = 5         # early stopping epochs (no improvement in val loss)\n",
    "GRAD_CLIP = 1.0\n",
    "FREEZE_BACKBONE = False\n",
    "\n",
    "# Model options\n",
    "#   Built-ins: 'resnet50' or 'efficientnet_b0'\n",
    "MODEL = 'resnet50'\n",
    "PRETRAINED = True\n",
    "\n",
    "# (Optional) Your own model:\n",
    "# Provide \"module.path:ClassName\" OR leave as None. Your module must be on PYTHONPATH.\n",
    "CUSTOM_MODEL = None          # e.g. 'my_models.vision:TinyNet'\n",
    "CUSTOM_WEIGHTS = None        # Optional: path to a state_dict or checkpoint\n",
    "\n",
    "# Inference config (used in the inference cell at the bottom)\n",
    "INFER_CHECKPOINT = None      # set after training, e.g. f\"{OUTPUT_DIR}/best_model.pt\"\n",
    "INFER_CLASS_INDEX = None     # optional override .json if you want\n",
    "INFER_INPUT = '/path/to/image_or_dir'  # image file or directory\n",
    "TOPK = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a498561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, math, random, time, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedShuffleSplit\n",
    "    HAVE_SKLEARN = True\n",
    "except Exception:\n",
    "    HAVE_SKLEARN = False\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    if seed is None or seed < 0:\n",
    "        return\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def timestamp():\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4497f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, rows: List[Tuple[str, str]], data_root: str, transform=None, class_to_idx: Optional[Dict[str, int]] = None):\n",
    "        self.data_root = Path(data_root)\n",
    "        self.rows = rows\n",
    "        self.transform = transform\n",
    "        labels = sorted({r[1] for r in rows})\n",
    "        if class_to_idx is None:\n",
    "            self.class_to_idx = {c: i for i, c in enumerate(labels)}\n",
    "        else:\n",
    "            self.class_to_idx = class_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_str, label = self.rows[idx]\n",
    "        img_path = Path(path_str)\n",
    "        if not img_path.is_absolute():\n",
    "            img_path = self.data_root / img_path\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        target = self.class_to_idx[label]\n",
    "        return img, target\n",
    "\n",
    "def read_csv_labels(csv_path: str) -> List[Tuple[str, str]]:\n",
    "    rows = []\n",
    "    with open(csv_path, newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        assert 'filepath' in reader.fieldnames and 'label' in reader.fieldnames, \\            \"CSV must contain 'filepath' and 'label' columns\"\n",
    "        for r in reader:\n",
    "            rows.append((r['filepath'], r['label']))\n",
    "    return rows\n",
    "\n",
    "def collect_folder_images(root: str) -> List[Tuple[str, str]]:\n",
    "    root = Path(root)\n",
    "    rows = []\n",
    "    for class_dir in sorted([p for p in root.iterdir() if p.is_dir()]):\n",
    "        label = class_dir.name\n",
    "        for img_path in class_dir.rglob('*'):\n",
    "            if img_path.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}:\n",
    "                rows.append((str(img_path), label))\n",
    "    if not rows:\n",
    "        raise RuntimeError(f\"No images found under {root}. Expected folder-per-class structure.\")\n",
    "    return rows\n",
    "\n",
    "def stratified_split(rows: List[Tuple[str, str]], val_size: float, test_size: float, seed: int):\n",
    "    labels = [r[1] for r in rows]\n",
    "    n = len(rows)\n",
    "    idx = list(range(n))\n",
    "    if HAVE_SKLEARN:\n",
    "        sss1 = StratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=seed)\n",
    "        trainval_idx, test_idx = next(sss1.split(idx, labels))\n",
    "        labels_trainval = [labels[i] for i in trainval_idx]\n",
    "        val_relative = val_size / (1 - test_size)\n",
    "        sss2 = StratifiedShuffleSplit(n_splits=1, test_size=val_relative, random_state=seed)\n",
    "        train_idx, val_idx = next(sss2.split(trainval_idx, labels_trainval))\n",
    "        train_idx = [trainval_idx[i] for i in train_idx]\n",
    "        val_idx = [trainval_idx[i] for i in val_idx]\n",
    "    else:\n",
    "        random.Random(seed).shuffle(idx)\n",
    "        n_test = int(math.floor(n * test_size))\n",
    "        n_val = int(math.floor(n * val_size))\n",
    "        test_idx = idx[:n_test]\n",
    "        val_idx = idx[n_test:n_test + n_val]\n",
    "        train_idx = idx[n_test + n_val:]\n",
    "    return train_idx, val_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956598e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_transforms(img_size: int, aug: bool):\n",
    "    train_tfms = [transforms.Resize((img_size, img_size))]\n",
    "    if aug:\n",
    "        train_tfms.extend([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05)], p=0.5),\n",
    "            transforms.RandomAffine(degrees=10, translate=(0.05, 0.05), scale=(0.95, 1.05)),\n",
    "        ])\n",
    "    train_tfms.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    eval_tfms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transforms.Compose(train_tfms), eval_tfms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4528629",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_model(num_classes: int, arch: str = 'resnet50', pretrained: bool = True, \n",
    "                freeze_backbone: bool = False, custom_model: Optional[str] = None, custom_weights: Optional[str] = None):\n",
    "    # Custom model path \"module.path:ClassName\"\n",
    "    if custom_model:\n",
    "        import importlib\n",
    "        try:\n",
    "            module_name, class_name = custom_model.split(\":\")\n",
    "            Mod = importlib.import_module(module_name)\n",
    "            ModelClass = getattr(Mod, class_name)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to import custom model '{custom_model}': {e}\")\n",
    "        # Try common constructor signatures\n",
    "        try:\n",
    "            model = ModelClass(num_classes=num_classes, pretrained=pretrained)\n",
    "        except TypeError:\n",
    "            try:\n",
    "                model = ModelClass(num_classes=num_classes)\n",
    "            except TypeError:\n",
    "                model = ModelClass()\n",
    "                # Heuristic: replace final layer if possible\n",
    "                if hasattr(model, 'fc') and isinstance(model.fc, nn.Linear):\n",
    "                    in_features = model.fc.in_features\n",
    "                    model.fc = nn.Linear(in_features, num_classes)\n",
    "                elif hasattr(model, 'classifier'):\n",
    "                    if isinstance(model.classifier, nn.Linear):\n",
    "                        in_features = model.classifier.in_features\n",
    "                        model.classifier = nn.Linear(in_features, num_classes)\n",
    "                    elif isinstance(model.classifier, nn.Sequential) and isinstance(model.classifier[-1], nn.Linear):\n",
    "                        in_features = model.classifier[-1].in_features\n",
    "                        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        if custom_weights:\n",
    "            sd = torch.load(custom_weights, map_location='cpu')\n",
    "            if isinstance(sd, dict) and 'state_dict' in sd:\n",
    "                sd = sd['state_dict']\n",
    "            model.load_state_dict(sd, strict=False)\n",
    "        return model\n",
    "\n",
    "    arch = arch.lower()\n",
    "    if arch == 'resnet50':\n",
    "        weights = models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n",
    "        model = models.resnet50(weights=weights)\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    elif arch == 'efficientnet_b0':\n",
    "        weights = models.EfficientNet_B0_Weights.IMAGENET1K_V1 if pretrained else None\n",
    "        model = models.efficientnet_b0(weights=weights)\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model arch: {arch}\")\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for name, p in model.named_parameters():\n",
    "            if 'fc' in name or 'classifier' in name:\n",
    "                p.requires_grad = True\n",
    "            else:\n",
    "                p.requires_grad = False\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69dabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append((correct_k.mul_(100.0 / batch_size).item()))\n",
    "        return res\n",
    "\n",
    "def save_json(obj: Dict, path: Path):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scaler, device, grad_clip):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    running_loss, running_top1 = 0.0, 0.0\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            if grad_clip > 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            if grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        top1, = accuracy(outputs, targets, topk=(1,))\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_top1 += top1 * images.size(0)\n",
    "    return running_loss / len(loader.dataset), running_top1 / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss, total_top1, total_top5 = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            top1, top5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            total_top1 += top1 * images.size(0)\n",
    "            total_top5 += top5 * images.size(0)\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, total_top1 / n, total_top5 / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70432e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_datasets(data_root: str, csv_path: Optional[str], img_size: int, aug: bool, \n",
    "                   val_size: float, test_size: float, seed: int):\n",
    "    if csv_path:\n",
    "        rows = read_csv_labels(csv_path)\n",
    "    else:\n",
    "        rows = collect_folder_images(data_root)\n",
    "\n",
    "    train_idx, val_idx, test_idx = stratified_split(rows, val_size, test_size, seed)\n",
    "\n",
    "    out = Path(OUTPUT_DIR)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    for name, indices in [('train', train_idx), ('val', val_idx), ('test', test_idx)]:\n",
    "        with open(out / f\"split_{name}.csv\", 'w', newline='') as f:\n",
    "            w = csv.writer(f); w.writerow(['filepath', 'label'])\n",
    "            for i in indices: w.writerow(rows[i])\n",
    "\n",
    "    train_tfms, eval_tfms = build_transforms(img_size, aug)\n",
    "    labels_sorted = sorted({r[1] for r in rows})\n",
    "    class_to_idx = {c: i for i, c in enumerate(labels_sorted)}\n",
    "\n",
    "    def subset(indices, train=False):\n",
    "        subrows = [rows[i] for i in indices]\n",
    "        return CSVDataset(subrows, data_root, transform=train_tfms if train else eval_tfms, class_to_idx=class_to_idx)\n",
    "\n",
    "    ds_train = subset(train_idx, train=True)\n",
    "    ds_val   = subset(val_idx, train=False)\n",
    "    ds_test  = subset(test_idx, train=False)\n",
    "    return ds_train, ds_val, ds_test, class_to_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== TRAIN ====\n",
    "set_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ds_train, ds_val, ds_test, class_to_idx = build_datasets(DATA_ROOT, CSV_PATH, IMG_SIZE, AUG, VAL_SIZE, TEST_SIZE, SEED)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "save_json(class_to_idx, Path(OUTPUT_DIR) / 'class_to_idx.json')\n",
    "\n",
    "model = build_model(num_classes=len(class_to_idx), arch=MODEL, pretrained=PRETRAINED, \n",
    "                    freeze_backbone=FREEZE_BACKBONE, custom_model=CUSTOM_MODEL, custom_weights=CUSTOM_WEIGHTS)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.AdamW(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP and device.type == 'cuda')\n",
    "\n",
    "train_loader = DataLoader(ds_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=True)\n",
    "val_loader   = DataLoader(ds_val,   batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "test_loader  = DataLoader(ds_test,  batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "steps_per_epoch = max(1, len(train_loader))\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS * steps_per_epoch)\n",
    "\n",
    "best_val = float('inf')\n",
    "best_path = Path(OUTPUT_DIR) / 'best_model.pt'\n",
    "history_path = Path(OUTPUT_DIR) / 'training_metrics.csv'\n",
    "\n",
    "with open(history_path, 'w') as f:\n",
    "    f.write('epoch,train_loss,train_top1,val_loss,val_top1,val_top5\\n')\n",
    "\n",
    "patience_counter = 0\n",
    "hist = {'epoch': [], 'train_loss': [], 'train_top1': [], 'val_loss': [], 'val_top1': [], 'val_top5': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss, train_top1 = train_one_epoch(model, train_loader, optimizer, scaler, device, GRAD_CLIP)\n",
    "    val_loss, val_top1, val_top5 = evaluate(model, val_loader, device)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    with open(history_path, 'a') as f:\n",
    "        f.write(f\"{epoch},{train_loss:.6f},{train_top1:.2f},{val_loss:.6f},{val_top1:.2f},{val_top5:.2f}\\n\")\n",
    "\n",
    "    hist['epoch'].append(epoch)\n",
    "    hist['train_loss'].append(train_loss)\n",
    "    hist['train_top1'].append(train_top1)\n",
    "    hist['val_loss'].append(val_loss)\n",
    "    hist['val_top1'].append(val_top1)\n",
    "    hist['val_top5'].append(val_top5)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train loss {train_loss:.4f} acc@1 {train_top1:.2f}% | val loss {val_loss:.4f} acc@1 {val_top1:.2f}% acc@5 {val_top5:.2f}% | {time.time()-t0:.1f}s\")\n",
    "\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        torch.save({'model_state': model.state_dict(),\n",
    "                    'arch': MODEL,\n",
    "                    'custom_model': CUSTOM_MODEL,\n",
    "                    'num_classes': len(class_to_idx),\n",
    "                    'class_to_idx': class_to_idx,\n",
    "                    'img_size': IMG_SIZE}, best_path)\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "# Final test on best ckpt\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "rebuild_model = build_model(ckpt.get('num_classes', len(ckpt['class_to_idx'])),\n",
    "                            arch=ckpt.get('arch','resnet50'),\n",
    "                            pretrained=False,\n",
    "                            custom_model=ckpt.get('custom_model'))\n",
    "rebuild_model.load_state_dict(ckpt['model_state'], strict=False)\n",
    "rebuild_model.to(device)\n",
    "\n",
    "test_loss, test_top1, test_top5 = evaluate(rebuild_model, test_loader, device)\n",
    "print(f\"Test: loss {test_loss:.4f} acc@1 {test_top1:.2f}% acc@5 {test_top5:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ad52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== PLOT METRICS ====\n",
    "# Single-axes plots without specifying colors or styles (per instructions)\n",
    "if len(hist['epoch']) > 0:\n",
    "    plt.figure()\n",
    "    plt.plot(hist['epoch'], hist['train_loss'], label='train_loss')\n",
    "    plt.plot(hist['epoch'], hist['val_loss'], label='val_loss')\n",
    "    plt.xlabel('epoch'); plt.ylabel('loss'); plt.legend(); plt.title('Loss vs Epoch')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(hist['epoch'], hist['train_top1'], label='train_acc1')\n",
    "    plt.plot(hist['epoch'], hist['val_top1'], label='val_acc1')\n",
    "    plt.xlabel('epoch'); plt.ylabel('acc@1 (%)'); plt.legend(); plt.title('Accuracy vs Epoch')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_checkpoint(checkpoint: str, device: torch.device):\n",
    "    ckpt = torch.load(checkpoint, map_localtion=device) if False else torch.load(checkpoint, map_location=device)\n",
    "    arch = ckpt.get('arch', 'resnet50')\n",
    "    custom_model = ckpt.get('custom_model', None)\n",
    "    num_classes = ckpt.get('num_classes', len(ckpt['class_to_idx']))\n",
    "    model = build_model(num_classes, arch=arch, pretrained=False, custom_model=custom_model)\n",
    "    model.load_state_dict(ckpt['model_state'], strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model, ckpt['class_to_idx'], ckpt.get('img_size', 224)\n",
    "\n",
    "def infer_on_path(model, class_to_idx: Dict[str, int], img_path: Path, img_size: int, device: torch.device, topk: int = 5):\n",
    "    inv_map = {v: k for k, v in class_to_idx.items()}\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = tfm(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        top_probs, top_idx = probs.topk(topk, dim=1)\n",
    "    preds = [(inv_map[i.item()], float(top_probs[0, j].item())) for j, i in enumerate(top_idx[0])]\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc657996",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== INFERENCE ====\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ckpt_path = INFER_CHECKPOINT or (Path(OUTPUT_DIR) / 'best_model.pt')\n",
    "assert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n",
    "\n",
    "model, class_to_idx, img_size = load_checkpoint(str(ckpt_path), device)\n",
    "\n",
    "if INFER_CLASS_INDEX and os.path.exists(INFER_CLASS_INDEX):\n",
    "    with open(INFER_CLASS_INDEX) as f:\n",
    "        class_to_idx = json.load(f)\n",
    "\n",
    "target = Path(INFER_INPUT)\n",
    "assert target.exists(), \"INFER_INPUT must be a valid file or directory\"\n",
    "\n",
    "paths = []\n",
    "if target.is_dir():\n",
    "    for p in sorted(target.rglob('*')):\n",
    "        if p.suffix.lower() in {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}:\n",
    "            paths.append(p)\n",
    "else:\n",
    "    paths = [target]\n",
    "\n",
    "for p in paths:\n",
    "    preds = infer_on_path(model, class_to_idx, p, img_size, device, topk=TOPK)\n",
    "    print(f\"{p} ->\")\n",
    "    for cls, prob in preds:\n",
    "        print(f\"  {cls:>20s}: {prob*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
